{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfb5249-3464-4172-a212-cd84c4cd0723",
   "metadata": {},
   "source": [
    "# VAME tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c4951-4ff1-4604-be12-0a7851aef6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import vame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329eb6ae-e676-4f30-a54a-90e8b179ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These paths have to be set manually\n",
    "working_directory = '.'\n",
    "project = 'my-vame-project'\n",
    "\n",
    "videos = ['video-1.mp4']\n",
    "poses_estimations = ['video-1.csv']\n",
    "\n",
    "# Step 1.1: Initialize your project\n",
    "config = vame.init_new_project(\n",
    "    project=project,\n",
    "    videos=videos,\n",
    "    poses_estimations=poses_estimations,\n",
    "    working_directory=working_directory,\n",
    "    videotype='.mp4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d8eb6-65ff-4a6c-af5f-b0100e4edaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2: Align your behavior videos egocentric and create training dataset\n",
    "# pose_ref_index: list of reference coordinate indices for alignment\n",
    "# Example: 0: snout, 1: forehand_left, 2: forehand_right, 3: hindleft, 4: hindright, 5: tail\n",
    "vame.egocentric_alignment(config, pose_ref_index=[0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be522b98-e9c0-4f1b-8b34-36f49c15c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your experiment is by design egocentrical (e.g. head-fixed experiment on treadmill etc)\n",
    "# you can use the following to convert your .csv to a .npy array, ready to train vame on it\n",
    "# vame.csv_to_numpy(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9a8d6-2304-4c3d-b952-834f504885c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.3: create the training set for the VAME model\n",
    "vame.create_trainset(config, check_parameter=False, pose_ref_index=[0,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dca7b-383c-4a20-b205-89ed6bc9c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train VAME\n",
    "vame.train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cf723-7c2f-4531-bd25-da454df626d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Evaluate model\n",
    "vame.evaluate_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276c865-bf49-4e30-8a61-facf35e59a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Segment motifs/pose\n",
    "vame.pose_segmentation(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea91412d",
   "metadata": {},
   "source": [
    "---\n",
    "#### The following are optional choices to create motif videos, communities/hierarchies of behavior and community videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: Create motif videos to get insights about the fine grained poses\n",
    "vame.motif_videos(config, videoType='.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0382c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: Create behavioural hierarchies via community detection\n",
    "vame.community(config, cut_tree=2, cohort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3425c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: Create community videos to get insights about behavior on a hierarchical scale\n",
    "vame.community_videos(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e4ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: Down projection of latent vectors and visualization via UMAP\n",
    "fig = vame.visualization(config, label=None) #options: label: None, \"motif\", \"community\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: Use the generative model (reconstruction decoder) to sample from\n",
    "# # the learned data distribution, reconstruct random real samples or visualize\n",
    "# # the cluster center for validation\n",
    "vame.generative_model(config, mode=\"centers\") #options: mode: \"sampling\", \"reconstruction\", \"centers\", \"motifs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7558661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Create a video of an egocentrically aligned mouse + path through\n",
    "# the community space (similar to our gif on github) to learn more about your representation\n",
    "# and have something cool to show around ;)\n",
    "# Note: This function is currently very slow. Once the frames are saved you can create a video\n",
    "# or gif via e.g. ImageJ or other tools\n",
    "vame.gif(config, pose_ref_index=[0,5], subtract_background=True, start=None,\n",
    "         length=500, max_lag=30, label='community', file_format='.mp4', crop_size=(300,300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe98abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
